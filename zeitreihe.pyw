# -*- coding: utf-8 -*-
"""Time Prediction.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1nJHBHpB3cv6xMYuanwOfrDpV-i_9zFNM
"""

#Installieren der Bibliotheken, falls in Google Coolab ausgeführt wird
#!pip install tensorflow #for prediction
#!pip install numpy  #for matrix multiplication
#!pip install pandas #define the data structures
#!pip install matplotlib #for visualization
#!pip install scikit-learn #for normalizing our data(scaling)

#importing the libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from tkinter import *
from tkinter.filedialog import askopenfilename
from tkinter import ttk
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

import io

	
# Funktion, um die Datensätze zu kreieren
#Input: Genutzter Datensatz
#window_size - wie viele Tage sollen berücksichtigt werden, um den nächsten Tag vorauszusagen 

def window_data(data, window_size):
	X = []
	y = []
	
	i = 0
	print(len(data))
	
	while (i + window_size) <= len(data) - 1:
		X.append(data[i:i+window_size]) # X enthält 93 Arrays mit je 7 Werten
		y.append(data[i+window_size]) # y enthält die Ergebnisse
		
		i += 1    # i geht bis 93, da window_size 7
	return X, y  


# Hauptsächliche Funktion. Erstellt das neuronale Netz und trainiert es. Das Ergebnis wird berechnet und zurückgegeben
def calculate(data_to_use):
	

	#Skalieren der Daten, sodass sie zwischen 0 und 1 liegen
	for i in range(len(data_to_use)):
		data_to_use[i] = data_to_use[i] /24

	scaled_data = data_to_use.reshape(-1,1)


	scaler = StandardScaler()

	# Einteilen der Daten mit der window_data Funktion
	X, y = window_data(scaled_data, 7) 
	#X: 93 Arrays, die Daten enthalten, die die Tage 7 bis 101 vorraussagen (Trainingsdaten)
	#y: Testdaten: Die letzten 93 Werte, die als Vergleichswerte genutzt werden
	# Aufteilung in Trainings- und Testset
	#import numpy as np
	X_train  = np.array(X[:99]) # 99: bis zur  99. Stelle 
	y_train = np.array(y[:99]) # :99 ab 99. Stelle
	# Die ersten 99 Werte werden zum Trainieren genutzt, um dann den 100. Wert vorauszusagen

	# Die letzten 7 Werte, um den 100. Tag vorauszusagen
	X_test = np.array(X[92:]) 
	y_test = np.array(y[92:])

	print("X_train size: {}".format(X_train.shape))
	print("y_train size: {}".format(y_train.shape))
	print("X_test size: {}".format(X_test.shape)) 
	print("y_test size: {}".format(y_test.shape))

	# Hier wird das Netz erstellt
	batch_size = 1 # Nummer der Datenreihen, die durchlaufen werden, bevor das Modell geupdatet wird
	window_size = 7 
	hidden_layer = 256 # Units der LSTM-Zelle
	clip_margin = 4 # Für Gradient clipping notwendig: wie genau soll das Netz sein?
	learning_rate = 0.001 # Wie viel ändert sich das Model bei fehlern?
	epochs = 200 # Nummer der kompletten Durchläufe bis zum Ende des Trainings

	
	#Definition der Platzhalter
	inputs = tf.placeholder(tf.float32, [batch_size, window_size, 1])
	targets = tf.placeholder(tf.float32, [batch_size, 1])


	# Gewichte der Zellen

	# Gewichte für die Input Layer
	#Es werden zufällig Gewichte verteilt:
	#truncates_normal: Zufällige Zahlen nahe 0
	#[1, hidden_layer]: Es soll ein Array von hidden_layer ausgegeben werden
	weights_input_gate = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev=0.05))
	weights_input_hidden = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev=0.05))
	bias_input = tf.Variable(tf.zeros([hidden_layer]))

	#Gewichte für das forgot gate
	weights_forget_gate = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev=0.05))
	weights_forget_hidden = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev=0.05))
	bias_forget = tf.Variable(tf.zeros([hidden_layer]))

	#Gewichte für das output gate
	weights_output_gate = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev=0.05))
	weights_output_hidden = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev=0.05))
	bias_output = tf.Variable(tf.zeros([hidden_layer]))

	#Gewichte für die memory cell
	weights_memory_cell = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev=0.05))
	weights_memory_cell_hidden = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev=0.05))
	bias_memory_cell = tf.Variable(tf.zeros([hidden_layer]))

	#Gewichte für die Output Layer
	weights_output = tf.Variable(tf.truncated_normal([hidden_layer, 1], stddev=0.05))
	bias_output_layer = tf.Variable(tf.zeros([1]))
	
	# Funktion, um die Zellen zu verknüpfen
	def LSTM_cell(input, output, state):
    
		input_gate = tf.sigmoid(tf.matmul(input, weights_input_gate) + tf.matmul(output, weights_input_hidden) + bias_input)
		
		forget_gate = tf.sigmoid(tf.matmul(input, weights_forget_gate) + tf.matmul(output, weights_forget_hidden) + bias_forget)
		
		output_gate = tf.sigmoid(tf.matmul(input, weights_output_gate) + tf.matmul(output, weights_output_hidden) + bias_output)
		
		memory_cell = tf.tanh(tf.matmul(input, weights_memory_cell) + tf.matmul(output, weights_memory_cell_hidden) + bias_memory_cell)
		
		state = state * forget_gate + input_gate * memory_cell
		
		output = output_gate * tf.tanh(state)
		return state, output
  
	# Definition der Schleife des Netzes
	outputs = []
	for i in range(batch_size): 
	  
		# Ausgangssituation sind nur Nullen
		batch_state = np.zeros([1, hidden_layer], dtype=np.float32) 
		batch_output = np.zeros([1, hidden_layer], dtype=np.float32)
		
		# an jedem Punkt wird durch den Input der Output bestimmt
		for ii in range(window_size):
			batch_state, batch_output = LSTM_cell(tf.reshape(inputs[i][ii], (-1, 1)), batch_state, batch_output)
			
		# Der letzte Output wird für die Prediction genutzt
		outputs.append(tf.matmul(batch_output, weights_output) + bias_output_layer)
	#outputs

	# Loss wird definiert
	losses = []

	for i in range(len(outputs)):
		losses.append(tf.losses.mean_squared_error(tf.reshape(targets[i], (-1, 1)), outputs[i]))
		
	loss = tf.reduce_mean(losses)

	#Optimierer wird erstellt
	gradients = tf.gradients(loss, tf.trainable_variables())
	clipped, _ = tf.clip_by_global_norm(gradients, clip_margin)
	optimizer = tf.train.AdamOptimizer(learning_rate)
	trained_optimizer = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))

	# Netz wird trainiert
	session = tf.Session()
	session.run(tf.global_variables_initializer())
	for i in range(epochs):
		traind_scores = []
		ii = 0
		epoch_loss = []
		while(ii + batch_size) <= len(X_train):
			X_batch = X_train[ii:ii+batch_size]
			y_batch = y_train[ii:ii+batch_size]
			
			o, c, _ = session.run([outputs, loss, trained_optimizer], feed_dict={inputs:X_batch, targets:y_batch})
			
			epoch_loss.append(c)
			traind_scores.append(o)
			ii += batch_size
		if (i % 30) == 0:
			print('Epoch {}/{}'.format(i, epochs), ' Current loss: {}'.format(np.mean(epoch_loss)))

	sup =[]
	for i in range(len(traind_scores)):
		for j in range(len(traind_scores[i])):
			sup.append(traind_scores[i][j][0])
		 
	
	# Daten werden berechnet
	tests = []
	i = 0
	while i+batch_size <= len(X_test):  
		o = session.run([outputs],feed_dict={inputs:X_test[i:i+batch_size]})
		i += batch_size
		tests.append(o)
	tests_new = []
	for i in range(len(tests)):
		for j in range(len(tests[i][0])):
			tests_new.append(tests[i][0][j])
	
	# Ergebnis wird festgehalten
	test_results = [] 
	for i in range(100):
		if i >= 99: 
			test_results.append(tests_new[i-99]) 
		else:
			test_results.append(None)	

	

	#Daten zurück auf 0 bis 24 formatieren
	result = test_results[99][0]*24
	return(result)
	
#def plot_network_predictions():
	#we now plot predictions from the network
#	plt.figure(figsize=(16, 7))
#	plt.title('Zeitanalyse')
#	plt.xlabel('Tage')
#	plt.ylabel('Uhrzeit')
#	plt.plot(unscaled_data, label='Original data')
#	plt.plot(unscaled_sup, label='Training data') # Was ist Training Data?
#	plt.plot(unscaled_test_results, label='Testing data')
#	plt.legend()
#	plt.show()




#Button_action definieren
def button_action():
	#öffnet ein Fenster des Dateiexplorers, um eine Datei auszuwählen
	button_for_upload.config(state=DISABLED)
	file_path = askopenfilename()
	if not file_path:
		button_for_upload.config(state=NORMAL)
	else:
		btc = pd.read_csv(file_path)
		# nur die benötigte Spalte auswählen
		data_to_use=btc['Startuhrzeit'].values
		result = calculate(data_to_use)
		button_for_upload.config(state=NORMAL)
		
		hour = int(result)		
		minutes_string = str(result).split(".")
		minutes = int(float( "0." + minutes_string[1][0:2])*60)
		
		time = "Die Standheizung soll um " + str(hour) + ":" + str(minutes) + " Uhr gestartet werden."
		
		popupmsg(time)

# Ausgabe des Ergebnisses in einem neuen Fenster
def popupmsg(msg):
    popup = Tk()
    popup.wm_title("Ergebnis")
    label = ttk.Label(popup, text=msg)
    label.pack()
    B1 = ttk.Button(popup, text="Okay", command=popup.destroy)
    B1.pack()
    popup.mainloop()
	
# Ein Fenster erstellen
fenster = Tk()
# Den Fenstertitel erstellen
fenster.title("Time Prediction")
fenster.geometry("500x100")
fenster.iconbitmap("car.ico")


file_path =  1
# Label und Buttons erstellen
button_for_upload_label = Label(fenster, text="Um die Voraussage zu starten bitte eine Datei hochladen:\n")
button_for_upload = Button(fenster, text="Datei angeben", command=button_action)
button_to_end = Button(fenster, text="Abbrechen", command=fenster.destroy)


# Komponenten dem Fenster hinzufügen
button_for_upload_label.pack()
button_for_upload.pack()
button_to_end.pack()

# In der Ereignisschleife auf Eingabe des Benutzers warten.
fenster.mainloop()